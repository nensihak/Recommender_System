{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the small dataset, IMPORTANT to read it exactly like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have added the line where the data will be transformed inside the function but I believe functions should not be tailored to one kind of dataframe, hence, this kind of manipulations should be done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=pd.read_csv('movieratings.csv',index_col=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the text files and transforming it to manageable dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to note that this changes to the 100K data should be done before using the functions, if you are testing for the small datasets just read the input_file accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining column names and reading the text file to get a dataframe\n",
    "rating_columns = ['User', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('u.data', sep='\\t', names=rating_columns)\n",
    "\n",
    "movie_columns = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "movies = pd.read_csv('u.item', sep='|', names=movie_columns, usecols=range(5),encoding='latin-1')\n",
    "\n",
    "#as we didn't have all the info in one file, we are merging two files to get movie names, user ids and ratings\n",
    "movie_ratings = pd.merge(movies, ratings)\n",
    "\n",
    "#for further manipulations we will need only these three columns and we need the index to be the user to be able to manipulate data easier\n",
    "movie_ratings=movie_ratings.loc[:,['title','User','rating']].set_index('User')\n",
    "\n",
    "#we also need to redesign the table to match the look of our smaller dataset so that it can be manipulated the same way, this structure is not randomly chosen, it fits the best manipulations with iterations and dataframe calculations\n",
    "data_big=movie_ratings.reset_index().groupby(['User', 'title'])['rating'].aggregate('first').unstack()\n",
    "\n",
    "#as opposed to smaller dataset we don't have names here, hence we transform the index to strings to have names of people rather than numbers even if it's only ids\n",
    "data_big.index=data_big.index.map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation algorithms\n",
    "\n",
    "# Pearson correlation coefficient for person1 and person2\n",
    "#\n",
    "# Output example:\n",
    "# 0.36514837167 \n",
    "\n",
    "def pearsonSimilarity(prefs,person1,person2):\n",
    "#dropping all the columns that are not mutually shared between two people\n",
    "    df_1=prefs.loc[[person1, person2],:].dropna(axis='columns')\n",
    "#finding correlation between the ratings that are left aka for the same movies both people watched\n",
    "    scores=df_1.loc[person1].corr(df_1.loc[person2])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets recommendations for person by using a weighted average\n",
    "# of every other user's rankings\n",
    "#\n",
    "# Output example:\n",
    "# [ (4.384834278347329, '2344: Sixth Sense, The (1999)'), \n",
    "#   (4.192391938482591, '2111: Saving Private Ryan (1998)'), \n",
    "#   (3.939493847382943, '3434: Forrest Gump (1994)'), \n",
    "#   (3.202023092992929, '3432: Blade Runner (1982)'), \n",
    "#   (2.929929928282816, '2342: Matrix, The (1999)'), \n",
    "#   (2.339434934923433, '3242: Shakespeare in Love (1998)'), \n",
    "#   (1.999329929223814, '2222: Shawshank Redemption, The (1994)')]\n",
    "\n",
    "\n",
    "def getRecommendations(prefs,person,similarity=pearsonSimilarity):\n",
    "#I am using np seterr as there are some NaNs and 0 values which might occur in the calculations only for the big dataset, which can be ignored and no how affect the end results, if it was crucial, certianly I would not have handled a warning by ignoring, but in Recommender we truly don't care about 0 values, which don't provide any valuable insight\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "#extracting the movies that the person watched in order to take it out later from the recommendations\n",
    "    person_watched=pd.DataFrame(prefs.loc[person,:].dropna()).drop(columns=[person])\n",
    "    movieratings=prefs\n",
    "    for title in person_watched.index:\n",
    "        movieratings=movieratings.drop(columns=title)\n",
    "    list_1=[]\n",
    "    list_2=[] \n",
    "#iterating through the movies that the person did not watch and adding correlations for all other users, excluding the person we are making the recommendations for\n",
    "    for i in movieratings.index:\n",
    "         if i != person:\n",
    "                list_1=(i,pearsonSimilarity(prefs,person,i))\n",
    "                list_2.append(list_1)    \n",
    "    list_2=pd.DataFrame(list_2)\n",
    "    list_2.columns=['User','Similarity']\n",
    "    \n",
    "#merging the correlations with ratings dataframe to perform calculations\n",
    "    main_data = pd.DataFrame.merge(list_2,movieratings, on='User',left_on=None, right_on=None)\n",
    "#multiplying similiarity with rating for every user and movie\n",
    "    for col in main_data.columns[2:]:\n",
    "         main_data[col] = np.where(main_data.loc[:,col]==\"NaN\",main_data.loc[:,col],main_data.loc[:,'Similarity']*main_data.loc[:,col])\n",
    "\n",
    "#because we need to handle the negative values in the correlation, I set a threshhold of 0 and cliped it, while also filling NAs with 0, this was the most efficient way not to distort the calculations, because in the formula we have sums, zeros would not cause any issues\n",
    "    main_data.iloc[:,1:]=main_data.iloc[:,1:].astype(\"float\").clip(0).fillna(0)\n",
    "    rankings=[]\n",
    "#here we needed to take into account similiarities only for the movies-wise, meaning if for movie X the user did not have a rating(or as we filled them the rating was 0), we need to skip taking their similiarities into account. Hence, I used the trick we did during our R class to have an extra column that allows us to manipulate the data easier. \n",
    "    for u in main_data.columns:\n",
    "        if (u != 'User') and (u!='Similarity'):\n",
    "            main_data.loc[main_data[u] !=0, 'Indicator'] = 1\n",
    "            main_data.loc[main_data[u] ==0, 'Indicator'] = 0\n",
    "#we create an if in case of movies that have 0 as a result, if there was no 'if' we would get an error that we can't divide by zero, hence, we need to take into account this specific cases\n",
    "            if sum(main_data['Similarity'].mul(main_data['Indicator'])) == 0:\n",
    "                r=0\n",
    "            else:\n",
    "                r=(sum(main_data[u])/sum(main_data['Similarity'].mul(main_data['Indicator'])))\n",
    "                if r>5:\n",
    "                    r=5\n",
    "#printing the rating and movie name and appending in the list\n",
    "            list_3=(r,u)\n",
    "            rankings.append(list_3)\n",
    "#I am applying the sorted function on the tuples and sorting by the rankings value from highest to lowest\n",
    "            rankings = sorted(rankings, key=lambda tup: tup[0],reverse=True)\n",
    "    return rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the best matches for person from the prefs dictionary. \n",
    "# Number of results and similarity function are optional params.\n",
    "#\n",
    "# Output example:\n",
    "# [ (0.8449111823071888, '2929: Amir'), \n",
    "#   (0.7476944573321964, '2934: Roger'), \n",
    "#   (0.7211183071882322, '2342: Raul'), \n",
    "#   (0.5393482847188896, '2344: Sarah'), \n",
    "#   (0.4793947394793984, '2343: Valentina')]\n",
    "\n",
    "def topMatches(prefs,person,n=5,similarity=pearsonSimilarity):\n",
    "#initializing lists where the results will be registered\n",
    "    list_1=[]\n",
    "    list_2=[]\n",
    "#basically this 'for loop' goes through all the people in our dataframe, excluding the person taken, pulls out the similiarities and if they are not NaNs then it appends them to the list, in this case we get NaNs when people don't have anything to do with each other hence their correlation is registered as NaN\n",
    "    for i in prefs.index:\n",
    "        if i != person:\n",
    "            list_1=(pearsonSimilarity(prefs,person,i),i)\n",
    "            if np.isnan(list_1[0])==False:\n",
    "                list_2.append(list_1)\n",
    "#here I am applying the sorted function on the tuples and sorting by the score value from highest to lowest, top 5\n",
    "        scores = sorted(list_2, key=lambda tup: tup[0],reverse=True)\n",
    "    return scores[0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I ran the functions on the Big Dataset, I noticed that the execution takes longer, which indeed is natural. On average it takes 130-140 times longer than on the small dataset. I have also times the execution times and the getRecommendations was the longest with about 20 seconds for the Big Dataset. All in all the recommenders worked so everyone is happy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
