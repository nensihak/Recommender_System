{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('exerciseCB.xlsx',sheet_name='CB - Simply Unary',header=1,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting the initial dataframe\n",
    "q_data = df.iloc[:20,:10]\n",
    "user_data=df.iloc[:20,13:17].fillna(0)\n",
    "answer_data=df.iloc[:20,18:22].fillna(0)\n",
    "copy_1=q_data.copy()\n",
    "copy_2=q_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Unary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User 1      User 2      User 3      User 4\n",
      "0  question16  question17   question5  question20\n",
      "1  question12   question2  question14  question19\n",
      "2   question6   question4  question19   question2\n",
      "3   question9  question14  question11   question3\n",
      "4   question1  question13   question8   question4\n"
     ]
    }
   ],
   "source": [
    "#I am going to iterate through users and follow the steps of \n",
    "#1) multiplying questions with user like/dislikes, \n",
    "#2) creating a user profile by summing the columns from step 1,\n",
    "#3) multiplying this user profile in step 2 with initial table of questions\n",
    "n=1\n",
    "results=pd.DataFrame()\n",
    "for i in user_data.columns:\n",
    "    for col in q_data.columns:\n",
    "        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col])\n",
    "        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n",
    "        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n",
    "    #Storing the results and then initiating empty lists again to have a fresh start for every user\n",
    "    results[n]=product\n",
    "    user_profile=[]\n",
    "    product=[]\n",
    "    n=n+1\n",
    "results.index=user_data.index \n",
    "top = pd.DataFrame()\n",
    "for r in results.columns:\n",
    "    ranking = results.loc[:,r].sort_values(ascending = False)\n",
    "    top[r]=ranking.index\n",
    "top.columns=user_data.columns\n",
    "#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\n",
    "print(top.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User 1      User 2      User 3      User 4\n",
      "0  question16  question17  question14  question20\n",
      "1   question6   question2   question5  question19\n",
      "2  question12   question4  question19   question2\n",
      "3   question9  question13  question11   question3\n",
      "4   question1  question14  question10   question4\n"
     ]
    }
   ],
   "source": [
    "#Exactly the same process with weights calculated row-wise to leverage on the number of topics in one question\n",
    "weights=1/copy_1.sum(axis=1)\n",
    "n=1\n",
    "results=pd.DataFrame()\n",
    "copy_2=q_data.copy()\n",
    "for i in user_data.columns:\n",
    "    for col in q_data.columns:\n",
    "        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*weights)\n",
    "        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n",
    "        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n",
    "    results[n]=product\n",
    "    user_profile=[]\n",
    "    product=[]\n",
    "    n=n+1\n",
    "#Simply sorting values and making it look clean by having same column names\n",
    "results.index=user_data.index \n",
    "top = pd.DataFrame()\n",
    "for r in results.columns:\n",
    "    ranking = results.loc[:,r].sort_values(ascending = False)\n",
    "    top[r]=ranking.index\n",
    "top.columns=user_data.columns\n",
    "#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\n",
    "print(top.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User 1      User 2      User 3      User 4\n",
      "0  question16  question17   question5  question20\n",
      "1   question6   question2  question14  question19\n",
      "2  question12   question4  question19   question2\n",
      "3   question1  question13  question11   question3\n",
      "4   question9  question14  question10   question4\n"
     ]
    }
   ],
   "source": [
    "#I am going to iterate through users and follow the steps of \n",
    "#1) multiplying questions with user like/dislikes, \n",
    "#2) getting weights based on number of topics in a question\n",
    "#3) getting IDF from frequency of occurance of a topic in all questions\n",
    "#4) to get the user profile I used IDF, weights and question data\n",
    "#5) multiplying this user profile in step 4 with initial table of questions\n",
    "list2=[]\n",
    "list3=[]\n",
    "#Getting IDF\n",
    "for i in q_data.index:\n",
    "    list1=1/sum(q_data.loc[i,:])\n",
    "    list2.append(list1)\n",
    "for i in q_data.columns:\n",
    "    list1=np.log10(20/sum(q_data.loc[:,i]))\n",
    "    list3.append(list1)\n",
    "list3=pd.DataFrame(list3).transpose()\n",
    "list3.columns=q_data.columns\n",
    "copy_2=q_data.copy()\n",
    "n=1\n",
    "results=pd.DataFrame()\n",
    "#Getting user_profile by using questions data IDF and weights\n",
    "for i in user_data.columns:\n",
    "    for col in q_data.columns:\n",
    "        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n",
    "        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n",
    "        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n",
    "    results[n]=product\n",
    "    user_profile=[]\n",
    "    product=[]\n",
    "    n=n+1\n",
    "#Simply sorting values and making it look clean by having same column names\n",
    "results.index=user_data.index \n",
    "top = pd.DataFrame()\n",
    "for r in results.columns:\n",
    "    rank = results.loc[:,r].sort_values(ascending = False)\n",
    "    top[r]=rank.index\n",
    "top.columns=user_data.columns\n",
    "#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\n",
    "print(top.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User 1      User 2      User 3      User 4\n",
      "0  question16  question17   question5  question14\n",
      "1   question6   question2  question14  question17\n",
      "2  question12   question4  question19   question2\n",
      "3   question1  question13  question11  question18\n",
      "4   question9  question14  question10   question5\n"
     ]
    }
   ],
   "source": [
    "#Initialising empty lists\n",
    "list2=[]\n",
    "list3=[]\n",
    "copy_1=q_data.copy()\n",
    "#Obtaining IDF by duplicating the same steps described in the previous method\n",
    "for i in q_data.index:\n",
    "    list1=1/sum(q_data.loc[i,:])\n",
    "    list2.append(list1)\n",
    "for i in q_data.columns:\n",
    "    list1=np.log10(20/sum(q_data.loc[:,i]))\n",
    "    list3.append(list1)\n",
    "list3=pd.DataFrame(list3).transpose()\n",
    "list3.columns=q_data.columns\n",
    "n=1\n",
    "results=pd.DataFrame()\n",
    "copy_2=q_data.copy()\n",
    "#Resolving cold start issue by returning the average of other users for the new user\n",
    "for i in user_data.columns:\n",
    "    if ((user_data.loc[:,i].mean())==0 and (user_data.loc[:,i].std())==0):\n",
    "        results.loc[:,n]=results.loc[:,results.columns!=n].mean(axis=1)\n",
    "    else:\n",
    "        copy_2=q_data.copy()\n",
    "        #Product of User Feedback, IDF, Weights and topic column \n",
    "        for col in q_data.columns:\n",
    "            copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n",
    "            user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n",
    "            product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n",
    "        results[n]=product\n",
    "        user_profile=[]\n",
    "        product=[]\n",
    "        n=n+1\n",
    "#Simply sorting values and making it look clean by having same column names\n",
    "results.index=user_data.index   \n",
    "top=pd.DataFrame()\n",
    "for r in results.columns:\n",
    "    ranking=results.loc[:,r].sort_values(ascending = False)\n",
    "    top[r]=ranking.index\n",
    "top.columns=user_data.columns\n",
    "print(top.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       User 1      User 2      User 3      User 4\n",
      "0  question16  question18   question5  question18\n",
      "1   question6  question20  question14  question14\n",
      "2  question12   question4  question19  question11\n",
      "3   question1   question7  question11  question20\n",
      "4   question9  question11  question10   question5\n"
     ]
    }
   ],
   "source": [
    "#In the Notebook I will explain the steps but the rationale can be found in the PDF\n",
    "#Rescaling the data in User Answers to logically merge it with User Feedback\n",
    "copy_3=q_data.copy()\n",
    "answer_data.columns=user_data.columns\n",
    "V1=pd.DataFrame(preprocessing.scale(answer_data))\n",
    "a_copy=answer_data.copy()\n",
    "\n",
    "#After scaling some data is distorted in particluar the zero values, hence I take two steps to recover them\n",
    "#1) I create a copy table which holds duplicates\n",
    "#2) I multiply values with that table to get values back because they hold important info to us\n",
    "for a in a_copy.columns:\n",
    "    for c in a_copy.index:\n",
    "        if (a_copy.loc[c,a]!=0):\n",
    "            a_copy.loc[c,a]=1\n",
    "m_product=np.multiply(V1,a_copy)\n",
    "m_product.columns=answer_data.columns\n",
    "m_product.index=answer_data.index\n",
    "a_data=m_product.copy()\n",
    "#I am following the logic from previous methods and multiplying the user data with answer data which was molded from user feedback and answers\n",
    "for n in user_data.columns:\n",
    "    for p in user_data.index:\n",
    "        if (user_data.loc[p,n]==1):\n",
    "            if (a_data.loc[p,n]!=0):\n",
    "                user_data.loc[p,n]=user_data.loc[p,n]*a_data.loc[p,n]\n",
    "                #Here I have a condition for negative values because if I don't treat them specially I will receive a positive based on two negatives, which does not correctly mirror the User Profile\n",
    "            elif(user_data.loc[p,n]== -1):\n",
    "                if (a_data.loc[p,n]<0):\n",
    "                    user_data.loc[p,n]=(-1)*user_data.loc[p,n]*a_data.loc[p,n]\n",
    "                if(answer_data.loc[p,n]>0):\n",
    "                    user_data.loc[p,n]=(-1)*user_data.loc[p,n]*a_data.loc[p,n]\n",
    "                #Here I have one more condition for zeros because we want to cover for cases when the person did not like/dislike but still gave a good answer, I wanted to count this in \n",
    "            elif(user_data.loc[p,n]== 0):\n",
    "                if (a_data.loc[p,n]!=0):\n",
    "                    user_data.loc[p,n]=a_data.loc[p,n]  \n",
    "#Initializing empty lists\n",
    "list2=[]\n",
    "list3=[]\n",
    "#Assigning weights based on frequency of occurence of a topic\n",
    "for i in q_data.index:\n",
    "    list1=1/sum(q_data.loc[i,:])\n",
    "    list2.append(list1)\n",
    "#Computing IDF\n",
    "for i in q_data.columns:\n",
    "    list1=np.log10(20/sum(q_data.loc[:,i]))\n",
    "    list3.append(list1)\n",
    "list3=pd.DataFrame(list3).transpose()\n",
    "list3.columns=q_data.columns\n",
    "n=1\n",
    "results=pd.DataFrame()\n",
    "#Here I make sure that for users who have a cold start, hence a st.dev of zero average is taken from other users' predictions\n",
    "for i in user_data.columns:\n",
    "    if ((user_data.loc[:,i].std())==0):\n",
    "        results.loc[:,n]=results.loc[:,results.columns!=n].mean(axis=1)\n",
    "    else:\n",
    "        copy_2=q_data.copy()\n",
    "        #Same methodology as in previous steps of using product of user profile(in this case a new one), weights and question data\n",
    "        for col in q_data.columns:\n",
    "            copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n",
    "            user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n",
    "            product=cosine_similarity(copy_3, user_profile).sum(axis=1)\n",
    "        results[n]=product\n",
    "        user_profile=[]\n",
    "        product=[]\n",
    "        n=n+1\n",
    "results.index=user_data.index \n",
    "#Simply sorting values and making it look clean by having same column names\n",
    "top=pd.DataFrame()\n",
    "for r in results.columns:\n",
    "    ranking=results.loc[:,r].sort_values(ascending = False)\n",
    "    top[r]=ranking.index\n",
    "top.columns=user_data.columns\n",
    "print(top.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
